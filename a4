import numpy as np
from sklearn.datasets import fetch_olivetti_faces
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.mixture import GaussianMixture
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Load the dataset
data = fetch_olivetti_faces()
X, y = data.data, data.target

# Split dataset into training, validation, and test sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

# Standardize the features for PCA
scaler = StandardScaler()
X_train_std = scaler.fit_transform(X_train)
X_val_std = scaler.transform(X_val)
X_test_std = scaler.transform(X_test)


pca = PCA(0.99, random_state=42)
X_train_pca = pca.fit_transform(X_train_std)
X_val_pca = pca.transform(X_val_std)
X_test_pca = pca.transform(X_test_std)

# Find best covariance type by comparing AIC/BIC scores
best_aic, best_gmm, best_cov_type = np.inf, None, ''
for cov_type in ['full', 'tied', 'diag', 'spherical']:
    gmm = GaussianMixture(n_components=40, covariance_type=cov_type, random_state=42)
    gmm.fit(X_train_pca)
    aic = gmm.aic(X_val_pca)
    if aic < best_aic:
        best_aic, best_gmm, best_cov_type = aic, gmm, cov_type

print(f"Best covariance type: {best_cov_type} with AIC: {best_aic}")

# Determine minimum clusters using BIC and plot results
bic_scores = []
for n in range(1, 41):  # Adjust range as needed
    gmm = GaussianMixture(n_components=n, covariance_type=best_cov_type, random_state=42)
    gmm.fit(X_train_pca)
    bic_scores.append(gmm.bic(X_val_pca))
    
plt.plot(range(1, 41), bic_scores, label='BIC')
plt.xlabel("Number of Clusters")
plt.ylabel("BIC Score")
plt.legend()
plt.show()

# Train final model with optimal number of clusters
optimal_clusters = np.argmin(bic_scores) + 1
final_gmm = GaussianMixture(n_components=optimal_clusters, covariance_type=best_cov_type, random_state=42)
final_gmm.fit(X_train_pca)

# Output hard clustering and soft clustering results
hard_clusters = final_gmm.predict(X_test_pca)
soft_clusters = final_gmm.predict_proba(X_test_pca)

print("Hard Clustering Assignments:", hard_clusters)
print("Soft Clustering Probabilities:\n", soft_clusters)

# Generate new faces
new_faces, _ = final_gmm.sample(10)
generated_faces = scaler.inverse_transform(pca.inverse_transform(new_faces))

fig, axs = plt.subplots(1, 10, figsize=(15, 2))
for i, face in enumerate(generated_faces):
    axs[i].imshow(face.reshape(64, 64), cmap='gray')
    axs[i].axis('off')
plt.show()

# Modify images to create anomalies
anomalies = X_test.copy()
anomalies[0] = np.flip(anomalies[0].reshape(64, 64), axis=1).flatten()  # Flip horizontally
anomalies[1] = np.roll(anomalies[1], 20)  # Rotate (by rolling pixels)
anomalies[2] = anomalies[2] * 0.5  # Darken

# Compare scores between normal and anomalous images
normal_scores = final_gmm.score_samples(X_test_pca)
anomaly_scores = final_gmm.score_samples(pca.transform(scaler.transform(anomalies)))

print("Normal Image Scores:", normal_scores)
print("Anomalous Image Scores:", anomaly_scores)
